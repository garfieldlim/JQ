{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cf47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, DataType, CollectionSchema, FieldSchema, Collection, Partition, utility\n",
    "from pymilvus import Milvus, DataType, Collection, MilvusException\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7856f",
   "metadata": {},
   "source": [
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fdf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ca411",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53183d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_list = [\n",
    "    'text_collection',\n",
    "    'author_collection',\n",
    "    'title_collection',\n",
    "    'contact_collection',\n",
    "    'name_collection',\n",
    "    'position_collection',\n",
    "    'department_collection',\n",
    "    'date_collection',\n",
    "]\n",
    "fields_list = [\n",
    "    'text',\n",
    "    'author',\n",
    "    'title',\n",
    "    'contact',\n",
    "    'name',\n",
    "    'position',\n",
    "    'department',\n",
    "    'date',\n",
    "]\n",
    "collections_dict = {\n",
    "    \"text_collection\": [\"uuid\", \"text_id\", \"text\", \"embeds\", \"media\", \"link\", \"partition_name\"],\n",
    "    \"author_collection\": [\"uuid\", \"author\", \"embeds\", \"partition_name\"],\n",
    "    \"title_collection\": [\"uuid\", \"title\", \"embeds\", \"partition_name\"],\n",
    "    \"date_collection\": [\"uuid\", \"date\", \"embeds\", \"partition_name\"],\n",
    "    \"contact_collection\": [\"uuid\", \"contact\", \"embeds\", \"partition_name\"],\n",
    "    \"department_collection\": [\"uuid\", \"department\", \"embeds\", \"partition_name\"],\n",
    "    \"name_collection\": [\"uuid\", \"name\", \"embeds\", \"partition_name\"],\n",
    "    \"position_collection\": [\"uuid\", \"position\", \"embeds\", \"partition_name\"]\n",
    "}\n",
    "\n",
    "partitions = {\n",
    "    \"documents_partition\": [\"text_collection\", \"author_collection\", \"title_collection\", \"date_collection\"],\n",
    "    \"social_posts_partition\": [\"text_collection\", \"date_collection\"],\n",
    "    \"contacts_partition\": [\"name_collection\", \"text_collection\", \"contact_collection\", \"department_collection\"],\n",
    "    \"people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\",\"department_collection\"],\n",
    "    \"usjr_documents_partition\": [\"text_collection\", \"title_collection\"],\n",
    "    \"scs_documents_partition\" : [\"text_collection\"],\n",
    "    \"religious_admin_people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63021bbb",
   "metadata": {},
   "source": [
    "Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e14b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the connection already exists\n",
    "if connections.has_connection('default'):\n",
    "    connections.remove_connection('default')  # Disconnect if it exists\n",
    "\n",
    "# Now, reconnect with your new configuration\n",
    "connections.connect(alias='default', host='localhost', port='19530')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa94f8",
   "metadata": {},
   "source": [
    "Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fefa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "    model = AutoModel.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "    # Prefix the text with 'query: '\n",
    "    text = 'query: ' + text\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Average pool the last hidden states and apply the attention mask\n",
    "    embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Convert tensor to list\n",
    "    embeddings_list = embeddings.tolist()\n",
    "\n",
    "    return embeddings_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde8963",
   "metadata": {},
   "source": [
    "Symbol remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b83715d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbcf0d",
   "metadata": {},
   "source": [
    "Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff960efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query):\n",
    "    return {'question1536': get_embedding(query.lower()),'question300': get_embedding(query.lower())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a584312",
   "metadata": {},
   "source": [
    "Search collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4fa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_collections(vectors, partition_names):\n",
    "    question1536=vectors['question1536']\n",
    "    question300=vectors['question300']\n",
    "    results_dict = {}\n",
    "    search_params = {\n",
    "    \"metric_type\": \"L2\",  # Distance metric, can be L2, IP (Inner Product), etc.\n",
    "    \"offset\": 0,}\n",
    "    for name in fields_list:\n",
    "        try:\n",
    "            if name == 'text':\n",
    "                collection = Collection(f\"{name}_collection\")\n",
    "                collection.load()\n",
    "                result = collection.search(\n",
    "                    data=[question1536],\n",
    "                    anns_field=\"embeds\",\n",
    "                    param=search_params,\n",
    "                    limit=10,\n",
    "                    partition_names=partition_names,\n",
    "                    output_fields=['uuid', 'text_id'],\n",
    "                    consistency_level=\"Strong\"\n",
    "                )\n",
    "                results_dict[name] = result\n",
    "            else:\n",
    "                collection = Collection(f\"{name}_collection\")\n",
    "                collection.load()\n",
    "                result = collection.search(\n",
    "                    data=[question300],\n",
    "                    anns_field=\"embeds\",\n",
    "                    param=search_params,\n",
    "                    limit=10,\n",
    "                    partition_names=partition_names,\n",
    "                    output_fields=['uuid'],\n",
    "                    consistency_level=\"Strong\"\n",
    "                )\n",
    "                results_dict[name] = result\n",
    "        except MilvusException as e:\n",
    "            if 'partition name' in str(e) and 'not found' in str(e):\n",
    "                print(f\"Partition '{partition_names}' not found in collection '{name}', skipping...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise e  # if it's a different kind of MilvusException, we still want to raise it\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c6bfe",
   "metadata": {},
   "source": [
    "Check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f830cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collection_dimension(collection):\n",
    "    collection_params = collection.schema\n",
    "    vector_field = [field for field in collection_params.fields if field.dtype == DataType.FLOAT_VECTOR][0]\n",
    "    print(f\"Dimension of vectors in collection '{collection.name}': {vector_field.params['dim']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d53de",
   "metadata": {},
   "source": [
    "Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results_dict):\n",
    "    json_results = {}\n",
    "\n",
    "    for collection_name, result in results_dict.items():\n",
    "        for query_hits in result:\n",
    "            for hit in query_hits:\n",
    "                if collection_name == 'text':\n",
    "                    id_field = 'entity_id'\n",
    "                    id_value = hit.entity.get('text_id')\n",
    "                else:\n",
    "                    id_field = 'entity_id'\n",
    "                    id_value = hit.id\n",
    "                \n",
    "                # Create the result dictionary\n",
    "                result_dict = {\n",
    "                    id_field: id_value,\n",
    "                    \"distance\": hit.distance,\n",
    "                    \"collection\": collection_name\n",
    "                }\n",
    "\n",
    "                # If the id_value is already in the results and the new distance is greater, skip\n",
    "                if id_value in json_results and json_results[id_value][\"distance\"] < hit.distance:\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, update/insert the result\n",
    "                json_results[id_value] = result_dict\n",
    "                \n",
    "            json_results_list = list(json_results.values())\n",
    "            json_results_sorted = sorted(json_results_list, key=lambda x: x['distance'])\n",
    "    \n",
    "    return json_results_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02567d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_results(json_results_sorted, partition_names):\n",
    "    # Load all collections beforehand\n",
    "    collections = {name: Collection(f\"{name}_collection\") for name in fields_list}\n",
    "\n",
    "    # Create a list of entity IDs for the query\n",
    "    entity_ids = [result[\"entity_id\"] for result in json_results_sorted]\n",
    "\n",
    "    # Preparing an empty dictionary for each field in the results\n",
    "    for result in json_results_sorted:\n",
    "        for name in fields_list:\n",
    "            result[name] = []\n",
    "\n",
    "    # Query for all relevant records at once\n",
    "    for name, collection in collections.items():\n",
    "        try:\n",
    "            # Prepare the query\n",
    "            output_fields = []\n",
    "            if name == 'text':\n",
    "                query_field = \"text_id\"\n",
    "                output_fields = [name, 'text_id']\n",
    "            else:\n",
    "                query_field = \"uuid\"\n",
    "                output_fields = [name]\n",
    "\n",
    "            query = f\"{query_field} in {entity_ids}\"\n",
    "\n",
    "            query_results = collection.query(\n",
    "                expr=query, \n",
    "                offset=0, \n",
    "                limit=len(entity_ids), \n",
    "                partition_names=[partition_names], \n",
    "                output_fields=output_fields, \n",
    "                consistency_level=\"Strong\"\n",
    "            )\n",
    "\n",
    "            # Append the results to the relevant fields in the results dictionary\n",
    "            for query_result in query_results:\n",
    "                for result in json_results_sorted:\n",
    "                    if (name == 'text' and result[\"entity_id\"] == query_result[\"text_id\"]) or (name != 'text' and result[\"entity_id\"] == query_result[\"uuid\"]):\n",
    "                        result[name].append(query_result[name])\n",
    "            final_results = []\n",
    "            for result in json_results_sorted:\n",
    "                obj = {}\n",
    "                for item in result:\n",
    "                    # If item is not 'entity_id' or 'distance' and the item's value is not empty\n",
    "                    if item not in ['entity_id', 'collection'] and result[item]:\n",
    "                        obj[item] = result[item]\n",
    "                final_results.append(obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with collection {name}: {str(e)}\")\n",
    "    return final_results[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4f87e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, string_json):\n",
    "    # Format the input as per the desired conversation format\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': \"\"\"You are Josenian Quiri. University of San Jose- Recoletos' general knowledge base assistant. Refer to yourself as JQ. If there are links, give the link as well.\"\"\"},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "        {'role': 'system', 'content': f'Here is the database JSON from your knowledge base (note: select only the correct answer): \\n{string_json[:4500]}'},\n",
    "        {'role': 'user', 'content': ''}\n",
    "    ]\n",
    "    \n",
    "    # Convert the conversation to a string\n",
    "    conversation_str = ''.join([f'{item[\"role\"]}: {item[\"content\"]}\\n' for item in conversation])\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-4\",\n",
    "      messages=conversation,\n",
    "      temperature=1,\n",
    "      max_tokens=1000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    # Extract the generated response from the API's response\n",
    "    generated_text = response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "    # Return the response\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84d78bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_attribute \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_model/clf_attribute.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m clf_partition \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier_model/clf_partition.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load encoders\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'"
     ]
    }
   ],
   "source": [
    "clf_attribute = joblib.load('classifier_model/clf_attribute.pkl')\n",
    "clf_partition = joblib.load('classifier_model/clf_partition.pkl')\n",
    "\n",
    "# load encoders\n",
    "le_attribute = joblib.load('classifier_model/le_attribute.pkl')\n",
    "le_partition = joblib.load('classifier_model/le_partition.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a35b0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_attribute \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_model/clf_attribute.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m clf_partition \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier_model/clf_partition.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load encoders\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'"
     ]
    }
   ],
   "source": [
    "clf_attribute = joblib.load('classifier_model/clf_attribute.pkl')\n",
    "clf_partition = joblib.load('classifier_model/clf_partition.pkl')\n",
    "\n",
    "# load encoders\n",
    "le_attribute = joblib.load('classifier_model/le_attribute.pkl')\n",
    "le_partition = joblib.load('classifier_model/le_partition.pkl')\n",
    "\n",
    "def predict_attribute(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_attribute.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_attribute.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_attribute.inverse_transform(ranked_classes)\n",
    "\n",
    "def predict_partition(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_partition.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_partition.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_partition.inverse_transform(ranked_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6f2635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_partitions(vectors):\n",
    "    return ['documents_partition', 'social_posts_partition',  'people_partition', \"contacts_partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dd95a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer():\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"You: \")\n",
    "            if not prompt:\n",
    "                print(\"No input provided. Try again.\")\n",
    "                continue\n",
    "            vectors = vectorize_query(prompt)\n",
    "            if vectors is None:\n",
    "                print(\"No vectors returned. Check your vectorize_query function.\")\n",
    "                continue\n",
    "            ranked_partitions = ranking_partitions(vectors['question300'])\n",
    "            if ranked_partitions is None:\n",
    "                print(\"No ranked_partitions returned. Check your ranking_partitions function.\")\n",
    "                continue\n",
    "            partition = 0\n",
    "            correct = 0\n",
    "            display(ranked_partitions)\n",
    "            display(ranked_partitions[partition])\n",
    "            while correct != 1:\n",
    "                results_dict = search_collections(vectors, [ranked_partitions[partition]])\n",
    "                if results_dict is None:\n",
    "                    print(\"No results returned. Check your search_collections function.\")\n",
    "                    break\n",
    "                json_results_sorted = process_results(results_dict)\n",
    "                if json_results_sorted is None:\n",
    "                    print(\"No sorted results returned. Check your process_results function.\")\n",
    "                    break\n",
    "                final_results = populate_results(json_results_sorted, ranked_partitions[partition])\n",
    "                if final_results is None:\n",
    "                    print(\"No final results returned. Check your populate_results function.\")\n",
    "                    break\n",
    "                string_json = json.dumps(final_results)\n",
    "                display(string_json)\n",
    "                generated_text = generate_response(prompt, string_json)\n",
    "                if generated_text is None:\n",
    "                    print(\"No response generated. Check your generate_response function.\")\n",
    "                    break\n",
    "                print(f\"JQ: {generated_text}\")\n",
    "                correct = input(\"Is the answer correct? 1-Y, 0-N: \")\n",
    "                if correct not in ['0', '1']:\n",
    "                    print(\"Invalid input. Try again.\")\n",
    "                elif partition <= 3 :\n",
    "                    partition = partition + 1\n",
    "                else:\n",
    "                    partition = 0\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00558ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: jovelyn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['documents_partition',\n",
       " 'social_posts_partition',\n",
       " 'people_partition',\n",
       " 'contacts_partition']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'documents_partition'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:33.861458', 'RPC error': '2023-07-31 00:22:33.866068'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:33.891723', 'RPC error': '2023-07-31 00:22:33.896823'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:33.918731', 'RPC error': '2023-07-31 00:22:33.922307'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:33.939349', 'RPC error': '2023-07-31 00:22:33.943560'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'contact', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'name', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'position', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'department', skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:35.429704', 'RPC error': '2023-07-31 00:22:35.435540'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:35.436234', 'RPC error': '2023-07-31 00:22:35.446689'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:35.447883', 'RPC error': '2023-07-31 00:22:35.453770'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:22:35.454277', 'RPC error': '2023-07-31 00:22:35.460354'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with collection contact: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection name: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection position: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection department: <MilvusException: (code=1, message=partition name documents_partition not found)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"distance\": 0.2737096846103668, \"text\": [\"The study aims to assess the extent of social media usage in talent acquisition in IT/BPM companies in Cebu and evaluate the insights of the applicants on the practice of using social media in character assessment for hiring decision. The quantitative method was employed to obtain data from two groups of respondents which constitute thirty hiring managers and ninety-six applicants. The study found that while hiring managers moderately practice social media background check to obtain additional information about the applicant, they seldom used it in hiring decisions because of the lack of formal or informal policy allowing or restricting the use of social media for that purpose. It is also found that there is a significant difference in perception between hiring managers and prospective applicants on usergenerated content on candidate social media profile. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/631\"], \"author\": [\"Jovelyn C Cuizon\"], \"title\": [\"Assessing Applicant Employability Using Social Media for Talent Acquisition and Recruitment in IT/BPM Companies\"], \"date\": [\"2019-06-06 June 06 2019\"]}, {\"distance\": 0.3424617052078247, \"text\": [\"This study aims to use social media data as corpus to assess person\\\\u00e2\\\\u20ac\\\\u2122s character to provide a preliminary background check on job seekers. It will provide recruiters an initial assessment of the candidates as well as supplementary information to support traditional recruitment and talent acquisition activities thereby reducing time and cost spent for character investigation. The application uses social media analytics to assign a social profile score. Unstructured text data are preprocessed to include only keywords which are relevant to the analysis. Word sense disambiguation is applied to determine the underlying meaning of the words. The bag-of-word is then checked for occurrence of associated words defined for each factor. Posts containing at least one occurrence of words associated with the factors are further tested for content polarity. Social character score is computed using proposed formula. The system recommends applicants based on skills and uses social character score for relevancy ranking of candidates relative to the job posts. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/245\"], \"author\": [\"Jovelyn Cuizon, Kent Ferolino\"], \"title\": [\"Social Media Character Assessment for Talent Selection using Natural Language Processing\"], \"date\": [\"2017-10-11 October 11 2017\"]}, {\"distance\": 0.40442341566085815, \"text\": [\"The study aimed to develop a software application to capture tourist activity information, extract movement patterns from the dataset through sequential pattern mining (SPM), and visualize spatiotemporal movement. Tourist activity information was captured through crowdsourced trajectory movements by scanning unique QR (Quick Response) codes for each visited tourist spots. The AprioriAll algorithm was used to find frequent trajectory patterns on tourist visits. The resulting maximal k-sequences and their subsequences represent the recommended trip itinerary. The spatial and temporal movements were visualized through a flow map and a heat map, respectively. The directed edges in the flow map show the recommended sequence of tourist sites to visit. The heat map shows the density of tourist visits in different areas at time intervals. The application was validated with selected tour planning experts to verify functional suitability, usability, and acceptability. Experimental results show positive indicators that the application met the users\\\\u00e2\\\\u20ac\\\\u2122 expectations. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/965\"], \"author\": [\"Maria Isabel R. Abucejo, Jovelyn C. Cuizon\"], \"title\": [\"Sequential Pattern Mining of Tourist Spatiotemporal Movement\"], \"date\": [\"2021-05-29 May 29 2021\"]}, {\"distance\": 0.4759845733642578, \"text\": [\"Forecasting plays a critical part in implementing effective tourism management strategies. However, the role of tourism forecasting is not extensively studied in the Philippines, which is a key tourism destination in Southeast Asia. To address such gap, this paper explores the dynamics of tourist demand in the Philippines through a case study. It illustrates the tourist arrival using a SARIMA model. Results show that the adopted methodology was able to capture the dynamics of the tourist demand in the Philippines. By providing lenses to the Philippine tourism case, this paper would help shed light to the gaps in the literature\\\\u00e2\\\\u20ac\\\\u2122s current understanding of tourism in the Philippines. Moreover, these findings would be beneficial for stakeholders in shaping policies, strategies, and other tourism initiatives. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/817\"], \"author\": [\"Severina P. Velos, Marivel B. Go, Glynne P. Bate, Elvira B. Joyohoy\"], \"title\": [\"A Seasonal Autoregressive Integrated Moving Average (SARIMA) Model to Forecasting Tourist Arrival in the Philippines: A Case Study in Moalboal, Cebu (Philippines)\"], \"date\": [\"2020-06-30 June 30 2020\"]}, {\"distance\": 0.4778299033641815, \"text\": [\"Stress has been identified as the antecedent for undesirable outcomes, such as depression and self-harm but there still lack of awareness on extent of its impact on academics most notably in a developing country like the Philippines. Thus, a multicenter and interdisciplinary study was employed on 501 students from six state universities in the Philippines. A questionnaire structured to assess the perception of the students as to the severity of identified stressors) as well as the prevalence of the coping mechanisms they adopt to overcome stress was used. The results indicated that academic stressors proved to be the main contributory factor to levels of distress among undergraduates. Educators and decision-makers should tailor regular student evaluations and programs that ensure the impediment of the sources of stress and other measures for further prevention. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/714\"], \"author\": [\"Glynne P. Bate, Marivel B. Go, Severina P. Velos\"], \"title\": [\"Top Stressors among Filipino Undergraduate Students: A Multicenter - Interdisciplinary Study\"], \"date\": [\"2019-12-31 December 31 2019\"]}, {\"distance\": 0.486061155796051, \"text\": [\"Science research culture in basic education can be associated with science investigatory projects (SIPs). In relation to this, the paper establishes a theory on the development of SIP culture in basic education utilizing theory generation through axiomatic deductive approach based on steps prescribed by R. Padua (personal communication, November 17, 2014) which creates the theory that basic education institutions contribute to high productivity in SIPs. Simply because humans and non-human resources along with quality Science instruction are needed to yield such productivity. These attributes, resources, and Science instruction foster SIP culture. These are crucial aspects as schools capacitate students and teachers, produce the SIPs, and eventually disseminate these projects in science fairs. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/1086\"], \"author\": [\"Joje Mar P. Sanchez\"], \"title\": [\"Development of Science Research Culture in Basic Education: A Theory Generation\"], \"date\": [\"2022-06-27 June 27 2022\"]}, {\"distance\": 0.48735591769218445, \"text\": [\"RMRJ is made up of internationally renowned scholars in the journal\\'s subject. They offer professional opinions on significant journal policies and content. The Editorial Team members improve and strengthen the quality, integrity, reputation, and sustainability of our publication through their field of expertise and substantial work as scientists and researchers.\\\\nThe journal adheres to the COPE Code of Conduct for Journal Editors to ensure fair and unbiased appraisal, confidentiality, non-competing interest compliance, and editorial duty and accountability for all submitted articles.\", \"Mr. Jesse Sagayno Susada, University of San Jose-Recoletos, Philippines\\\\nMrs Milagros B. Baclayon, University of San Jose-Recoletos, Philippines\\\\nMiss Chanine F. Sevilla, University of San Jose-Recoletos, Philippines\\\\nMrs. Ingrid S. Ramos, University of San Jose-Recoletos, Philippines\", \"Dr. Agnes C. Sequino, University of San Jose-Recoletos, Philippines\\\\nDr. Ravindra C. Joshi, CABI-SEA, Malaysia\\\\nDr. Erwin Faller, San Pedro College, Philippines\\\\nEnrique G. Oracion, Ph.D., Silliman University, Philippines\\\\nDr. Jay P. Picardal, Cebu Normal University, Philippines\\\\nDr. Roger Lincoln Radix, St. George University, University Center Grenada, United States\\\\nDr. Gaurang Dattubhai Rami, Veer Narmad South Gujarat University, India\\\\nDr. Brian A. Vasquez, Majmaah University, Kingdom of Saudi Arabia\\\\nDr. Diane Bandow, Troy University, United States of America\\\\nDr. Glenn G. Pajares, University of San Jose-Recoletos, Philippines\\\\nDr. William J. Heisler, Troy University, United States of America\\\\nDr. Lanndon A. Ocampo, Cebu Technological University, Philippines\\\\nDr. Rene M. Odendaal, University of South Africa, South Africa\\\\nDr. Robert Halliman, Austin Peay State University, United States of America\\\\nDr. Mohammed Seghir Halimi, University of Kasdi Merbah Ouargla, Algeria\\\\nDr. Djuwari Djuwari, Universitas Nahdlatul Ulama Surabaya (UNUSA), Indonesia\\\\nAssist. Prof. Dr. Faruk Kokoglu, Mugla University, Turkey\\\\nDr. Nathaniel A. Adebayo, The Polytechnic, Ibadan, Nigeria\\\\nDr. Anthony M. Penaso, Caraga State University, Philippines\\\\nDr. Slawomir Jablonski, Institute of Psychology, Adam Mickiewicz University, Poznan, Poland\\\\nRoberto B. Corcino, Ph.D., Cebu Normal University, Philippines\\\\nJovito C. Anito Jr., Jose Rizal Memorial State University, Philippines\\\\nImee C. Acosta, Ph.D., Virginia Commonwealth University Qatar, Doha, Qatar\\\\nBonjovi H. Hajan, Jos\\\\u00c3\\\\u00a9 Rizal University, Philippines\\\\nRoy Francis Navea, Ph.D., De La Salle University, Philippines\\\\nJony V. Berjes, DM, The University of Suwon, South Korea\", \"Jessica Magallon- Avenido, Ph.D., University of San Jose-Recoletos, Philippines\"]}, {\"distance\": 0.49304264783859253, \"text\": [\"This paper inquires into the problems concerning Filipino values and moral norms. Based on the interviews with the social science and philosophy scholars and the youth leaders in the Philippines, the study identifies the following problems: Filipino identity, distortion and dysfunctionalization, manifold ambivalence, dissonance, false justification and misuse, cynicism, and decline of moral courage. Analyzed based on Hans Kelsen\\\\u00e2\\\\u20ac\\\\u2122s concept of validity and efficacy, the problems prove to be radical given that the purported Filipino values system is actually a chaotic constellation of competing and conflicting pre-colonial, colonial, and postcolonial normative paradigms. Distorted, ambivalent, and dysfunctionalized, Filipino values and norms fail to provide effective normative guidelines. The proposed antidote of moral and values education is bound to be futile in the face of a severely mutilated social conscience. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/1211\"], \"author\": [\"Jiolito L. Benitez\"], \"title\": [\"An Inquiry into the Problems Concerning Filipino Values and Norms\"], \"date\": [\"2022-05-27 May 27 2022\"]}, {\"distance\": 0.4990939795970917, \"text\": [\"Research is a priority in higher education institutions. Considering that the development of research culture is highly influenced by the paradigm by which institutions operate on, this paper sought to identify the configuration of the research culture. The researcher conducted a narrative inquiry with key informants from seven reputable teacher education institutions in Region VII and coded the interview transcripts with the aid of NVIVO 11.3.2. Using Thematic Analysis (Braun and Clarke, 2006), three overarching themes on the configuration of the research culture, with its corresponding attributes and characteristics, were identified. The results showed that the research culture consists of observable and measurable inputs and outputs that interact in internal and external dynamics that are developmental and systemic, leading to standardized and contextualized practices in a teacher education institution. Therefore, research culture is an investment, a process, and a norm as it is evidence-based, dynamic, and distinct to the academic institution. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/861\"], \"author\": [\"Michelle Mae Olvido\"], \"title\": [\"Configuration of Research Culture: Investment, Process, and Norm\"], \"date\": [\"2020-12-28 December 28 2020\"]}, {\"distance\": 0.5005134344100952, \"text\": [\"This study is an empirical validation of the economic and environmental parameters of tourism in the Philippines. The determined tourism factors to be affected by tropical depressions are: tourist arrivals, tourism employment rate and travel price. These factors had varied impacts on economic growth in terms of annual gross domestic product through: exchange rate, inflation rate, consumer price index, and employment rate. The researcher utilized a path analysis to determine which among the tourism factors had a robust effect by the tropical depression. This study also concludes the effects (both direct and indirect) of predictors to GDP. A model was established from the predictors (x) to its response (y). To demonstrate the paths from the origin to the end, a computed path coefficients were generated. The summation of value effects was determined and an effect model was established. Findings revealed that travel price factor was a prevalent element that influenced the volatility of GDP and not more on the tourist arrivals and tourism employment rate. Tourists, regardless of nationality, prefer to spend more their money, enjoy and mesmerize the natural beauty of Philippines even with the occurrence of natural disturbances. Hence, this study implies on strengthening the policy of fixed prices on travels and all in-country cost, developing more the outdoor and indoor tourism industry, the Philippines infrastructure to sustain and enhance tourism services. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/23\"], \"author\": [\"Helmae N. Etulle-Tapanan\"], \"title\": [\"Path Analysis of Climate and Tourism to the Economic Growth in the Philippines\"], \"date\": [\"2015-06-30 June 30 2015\"]}]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Incorrect API key provided: sk-mwbr7***************************************gOYP. You can find your API key at https://platform.openai.com/account/api-keys.\n"
     ]
    }
   ],
   "source": [
    "question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7d616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc6ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285947f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75220d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
