{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cf47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, DataType, CollectionSchema, FieldSchema, Collection, Partition, utility\n",
    "from pymilvus import Milvus, DataType, Collection, MilvusException\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7856f",
   "metadata": {},
   "source": [
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fdf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ca411",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53183d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_list = [\n",
    "    'text_collection',\n",
    "    'author_collection',\n",
    "    'title_collection',\n",
    "    'contact_collection',\n",
    "    'name_collection',\n",
    "    'position_collection',\n",
    "    'department_collection',\n",
    "    'date_collection',\n",
    "]\n",
    "fields_list = [\n",
    "    'text',\n",
    "    'author',\n",
    "    'title',\n",
    "    'contact',\n",
    "    'name',\n",
    "    'position',\n",
    "    'department',\n",
    "    'date',\n",
    "]\n",
    "collections_dict = {\n",
    "    \"text_collection\": [\"uuid\", \"text_id\", \"text\", \"embeds\", \"media\", \"link\", \"partition_name\"],\n",
    "    \"author_collection\": [\"uuid\", \"author\", \"embeds\", \"partition_name\"],\n",
    "    \"title_collection\": [\"uuid\", \"title\", \"embeds\", \"partition_name\"],\n",
    "    \"date_collection\": [\"uuid\", \"date\", \"embeds\", \"partition_name\"],\n",
    "    \"contact_collection\": [\"uuid\", \"contact\", \"embeds\", \"partition_name\"],\n",
    "    \"department_collection\": [\"uuid\", \"department\", \"embeds\", \"partition_name\"],\n",
    "    \"name_collection\": [\"uuid\", \"name\", \"embeds\", \"partition_name\"],\n",
    "    \"position_collection\": [\"uuid\", \"position\", \"embeds\", \"partition_name\"]\n",
    "}\n",
    "\n",
    "partitions = {\n",
    "    \"documents_partition\": [\"text_collection\", \"author_collection\", \"title_collection\", \"date_collection\"],\n",
    "    \"social_posts_partition\": [\"text_collection\", \"date_collection\"],\n",
    "    \"contacts_partition\": [\"name_collection\", \"text_collection\", \"contact_collection\", \"department_collection\"],\n",
    "    \"people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\",\"department_collection\"],\n",
    "    \"usjr_documents_partition\": [\"text_collection\", \"title_collection\"],\n",
    "    \"scs_documents_partition\" : [\"text_collection\"],\n",
    "    \"religious_admin_people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63021bbb",
   "metadata": {},
   "source": [
    "Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e14b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the connection already exists\n",
    "if connections.has_connection('default'):\n",
    "    connections.remove_connection('default')  # Disconnect if it exists\n",
    "\n",
    "# Now, reconnect with your new configuration\n",
    "connections.connect(alias='default', host='localhost', port='19530')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa94f8",
   "metadata": {},
   "source": [
    "Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fefa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "    model = AutoModel.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "    # Prefix the text with 'query: '\n",
    "    text = 'query: ' + text\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Average pool the last hidden states and apply the attention mask\n",
    "    embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Convert tensor to list\n",
    "    embeddings_list = embeddings.tolist()\n",
    "\n",
    "    return embeddings_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde8963",
   "metadata": {},
   "source": [
    "Symbol remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83715d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbcf0d",
   "metadata": {},
   "source": [
    "Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff960efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query):\n",
    "    return {'question1536': get_embedding(query.lower()),'question300': get_embedding(query.lower())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a584312",
   "metadata": {},
   "source": [
    "Search collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4fa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_collections(vectors, partition_names):\n",
    "    question1536=vectors['question1536']\n",
    "    question300=vectors['question300']\n",
    "    results_dict = {}\n",
    "    search_params = {\n",
    "    \"metric_type\": \"L2\",  # Distance metric, can be L2, IP (Inner Product), etc.\n",
    "    \"offset\": 0,}\n",
    "    for name in fields_list:\n",
    "        try:\n",
    "            if name == 'text':\n",
    "                collection = Collection(f\"{name}_collection\")\n",
    "                collection.load()\n",
    "                result = collection.search(\n",
    "                    data=[question1536],\n",
    "                    anns_field=\"embeds\",\n",
    "                    param=search_params,\n",
    "                    limit=10,\n",
    "                    partition_names=partition_names,\n",
    "                    output_fields=['uuid', 'text_id'],\n",
    "                    consistency_level=\"Strong\"\n",
    "                )\n",
    "                results_dict[name] = result\n",
    "            else:\n",
    "                collection = Collection(f\"{name}_collection\")\n",
    "                collection.load()\n",
    "                result = collection.search(\n",
    "                    data=[question300],\n",
    "                    anns_field=\"embeds\",\n",
    "                    param=search_params,\n",
    "                    limit=10,\n",
    "                    partition_names=partition_names,\n",
    "                    output_fields=['uuid'],\n",
    "                    consistency_level=\"Strong\"\n",
    "                )\n",
    "                results_dict[name] = result\n",
    "        except MilvusException as e:\n",
    "            if 'partition name' in str(e) and 'not found' in str(e):\n",
    "                print(f\"Partition '{partition_names}' not found in collection '{name}', skipping...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise e  # if it's a different kind of MilvusException, we still want to raise it\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c6bfe",
   "metadata": {},
   "source": [
    "Check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f830cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collection_dimension(collection):\n",
    "    collection_params = collection.schema\n",
    "    vector_field = [field for field in collection_params.fields if field.dtype == DataType.FLOAT_VECTOR][0]\n",
    "    print(f\"Dimension of vectors in collection '{collection.name}': {vector_field.params['dim']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d53de",
   "metadata": {},
   "source": [
    "Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results_dict):\n",
    "    json_results = {}\n",
    "\n",
    "    for collection_name, result in results_dict.items():\n",
    "        for query_hits in result:\n",
    "            for hit in query_hits:\n",
    "                if collection_name == 'text':\n",
    "                    id_field = 'entity_id'\n",
    "                    id_value = hit.entity.get('text_id')\n",
    "                else:\n",
    "                    id_field = 'entity_id'\n",
    "                    id_value = hit.id\n",
    "                \n",
    "                # Create the result dictionary\n",
    "                result_dict = {\n",
    "                    id_field: id_value,\n",
    "                    \"distance\": hit.distance,\n",
    "                    \"collection\": collection_name\n",
    "                }\n",
    "\n",
    "                # If the id_value is already in the results and the new distance is greater, skip\n",
    "                if id_value in json_results and json_results[id_value][\"distance\"] < hit.distance:\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, update/insert the result\n",
    "                json_results[id_value] = result_dict\n",
    "                \n",
    "            json_results_list = list(json_results.values())\n",
    "            json_results_sorted = sorted(json_results_list, key=lambda x: x['distance'])\n",
    "    \n",
    "    return json_results_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02567d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_results(json_results_sorted, partition_names):\n",
    "    # Load all collections beforehand\n",
    "    collections = {name: Collection(f\"{name}_collection\") for name in fields_list}\n",
    "\n",
    "    # Create a list of entity IDs for the query\n",
    "    entity_ids = [result[\"entity_id\"] for result in json_results_sorted]\n",
    "\n",
    "    # Preparing an empty dictionary for each field in the results\n",
    "    for result in json_results_sorted:\n",
    "        for name in fields_list:\n",
    "            result[name] = []\n",
    "\n",
    "    # Query for all relevant records at once\n",
    "    for name, collection in collections.items():\n",
    "        try:\n",
    "            # Prepare the query\n",
    "            output_fields = []\n",
    "            if name == 'text':\n",
    "                query_field = \"text_id\"\n",
    "                output_fields = [name, 'text_id']\n",
    "            else:\n",
    "                query_field = \"uuid\"\n",
    "                output_fields = [name]\n",
    "\n",
    "            query = f\"{query_field} in {entity_ids}\"\n",
    "\n",
    "            query_results = collection.query(\n",
    "                expr=query, \n",
    "                offset=0, \n",
    "                limit=len(entity_ids), \n",
    "                partition_names=[partition_names], \n",
    "                output_fields=output_fields, \n",
    "                consistency_level=\"Strong\"\n",
    "            )\n",
    "\n",
    "            # Append the results to the relevant fields in the results dictionary\n",
    "            for query_result in query_results:\n",
    "                for result in json_results_sorted:\n",
    "                    if (name == 'text' and result[\"entity_id\"] == query_result[\"text_id\"]) or (name != 'text' and result[\"entity_id\"] == query_result[\"uuid\"]):\n",
    "                        result[name].append(query_result[name])\n",
    "            final_results = []\n",
    "            for result in json_results_sorted:\n",
    "                obj = {}\n",
    "                for item in result:\n",
    "                    # If item is not 'entity_id' or 'distance' and the item's value is not empty\n",
    "                    if item not in ['entity_id', 'collection'] and result[item]:\n",
    "                        obj[item] = result[item]\n",
    "                final_results.append(obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with collection {name}: {str(e)}\")\n",
    "    return final_results[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f87e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, string_json):\n",
    "    # Format the input as per the desired conversation format\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': \"\"\"You are Josenian Quiri. University of San Jose- Recoletos' general knowledge base assistant. Refer to yourself as JQ. If there are links, give the link as well.\"\"\"},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "        {'role': 'system', 'content': f'Here is the database JSON from your knowledge base (note: select only the correct answer): \\n{string_json[:4500]}'},\n",
    "        {'role': 'user', 'content': ''}\n",
    "    ]\n",
    "    \n",
    "    # Convert the conversation to a string\n",
    "    conversation_str = ''.join([f'{item[\"role\"]}: {item[\"content\"]}\\n' for item in conversation])\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-4\",\n",
    "      messages=conversation,\n",
    "      temperature=1,\n",
    "      max_tokens=1000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    # Extract the generated response from the API's response\n",
    "    generated_text = response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "    # Return the response\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84d78bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf_attribute = joblib.load('lib/classifier_model/clf_attribute.pkl')\n",
    "# clf_partition = joblib.load('lib/classifier_model/clf_partition.pkl')\n",
    "\n",
    "# # load encoders\n",
    "# le_attribute = joblib.load('lib/classifier_model/le_attribute.pkl')\n",
    "# le_partition = joblib.load('lib/classifier_model/le_partition.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a35b0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_attribute \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_model/clf_attribute.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m clf_partition \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier_model/clf_partition.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load encoders\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classifier_model/clf_attribute.pkl'"
     ]
    }
   ],
   "source": [
    "clf_attribute = joblib.load('classifier_model/clf_attribute.pkl')\n",
    "clf_partition = joblib.load('classifier_model/clf_partition.pkl')\n",
    "\n",
    "# load encoders\n",
    "le_attribute = joblib.load('classifier_model/le_attribute.pkl')\n",
    "le_partition = joblib.load('classifier_model/le_partition.pkl')\n",
    "\n",
    "def predict_attribute(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_attribute.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_attribute.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_attribute.inverse_transform(ranked_classes)\n",
    "\n",
    "def predict_partition(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_partition.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_partition.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_partition.inverse_transform(ranked_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f2635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_partitions(vectors):\n",
    "    return ['documents_partition', 'social_posts_partition',  'people_partition', \"contacts_partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd95a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer():\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"You: \")\n",
    "            if not prompt:\n",
    "                print(\"No input provided. Try again.\")\n",
    "                continue\n",
    "            vectors = vectorize_query(prompt)\n",
    "            if vectors is None:\n",
    "                print(\"No vectors returned. Check your vectorize_query function.\")\n",
    "                continue\n",
    "            ranked_partitions = ranking_partitions(vectors['question300'])\n",
    "            if ranked_partitions is None:\n",
    "                print(\"No ranked_partitions returned. Check your ranking_partitions function.\")\n",
    "                continue\n",
    "            partition = 0\n",
    "            correct = 0\n",
    "            display(ranked_partitions)\n",
    "            display(ranked_partitions[partition])\n",
    "            while correct != 1:\n",
    "                results_dict = search_collections(vectors, [ranked_partitions[partition]])\n",
    "                if results_dict is None:\n",
    "                    print(\"No results returned. Check your search_collections function.\")\n",
    "                    break\n",
    "                json_results_sorted = process_results(results_dict)\n",
    "                if json_results_sorted is None:\n",
    "                    print(\"No sorted results returned. Check your process_results function.\")\n",
    "                    break\n",
    "                final_results = populate_results(json_results_sorted, ranked_partitions[partition])\n",
    "                if final_results is None:\n",
    "                    print(\"No final results returned. Check your populate_results function.\")\n",
    "                    break\n",
    "                string_json = json.dumps(final_results)\n",
    "                display(string_json)\n",
    "                generated_text = generate_response(prompt, string_json)\n",
    "                if generated_text is None:\n",
    "                    print(\"No response generated. Check your generate_response function.\")\n",
    "                    break\n",
    "                print(f\"JQ: {generated_text}\")\n",
    "                correct = input(\"Is the answer correct? 1-Y, 0-N: \")\n",
    "                if correct not in ['0', '1']:\n",
    "                    print(\"Invalid input. Try again.\")\n",
    "                elif partition <= 3 :\n",
    "                    partition = partition + 1\n",
    "                else:\n",
    "                    partition = 0\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00558ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What did Jovelyn Cuizon author?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['documents_partition',\n",
       " 'social_posts_partition',\n",
       " 'people_partition',\n",
       " 'contacts_partition']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'documents_partition'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:29.837901', 'RPC error': '2024-02-12 14:52:29.846739'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'contact', skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:31.899288', 'RPC error': '2024-02-12 14:52:31.907348'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'name', skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:33.956994', 'RPC error': '2024-02-12 14:52:33.962156'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'position', skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:35.521335', 'RPC error': '2024-02-12 14:52:35.528972'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'department', skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:38.801455', 'RPC error': '2024-02-12 14:52:38.806823'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:38.807718', 'RPC error': '2024-02-12 14:52:38.813496'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:38.814020', 'RPC error': '2024-02-12 14:52:38.817197'}>\n",
      "RPC error: [query], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2024-02-12 14:52:38.817643', 'RPC error': '2024-02-12 14:52:38.822195'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with collection contact: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection name: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection position: <MilvusException: (code=1, message=partition name documents_partition not found)>\n",
      "Error with collection department: <MilvusException: (code=1, message=partition name documents_partition not found)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"distance\": 0.06181073561310768, \"text\": [\"The study aims to assess the extent of social media usage in talent acquisition in IT/BPM companies in Cebu and evaluate the insights of the applicants on the practice of using social media in character assessment for hiring decision. The quantitative method was employed to obtain data from two groups of respondents which constitute thirty hiring managers and ninety-six applicants. The study found that while hiring managers moderately practice social media background check to obtain additional information about the applicant, they seldom used it in hiring decisions because of the lack of formal or informal policy allowing or restricting the use of social media for that purpose. It is also found that there is a significant difference in perception between hiring managers and prospective applicants on usergenerated content on candidate social media profile. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/631\"], \"author\": [\"Jovelyn C Cuizon\"], \"title\": [\"Assessing Applicant Employability Using Social Media for Talent Acquisition and Recruitment in IT/BPM Companies\"], \"date\": [\"2019-06-06 June 06 2019\"]}, {\"distance\": 0.14063630998134613, \"text\": [\"This study aims to use social media data as corpus to assess person\\\\u00e2\\\\u20ac\\\\u2122s character to provide a preliminary background check on job seekers. It will provide recruiters an initial assessment of the candidates as well as supplementary information to support traditional recruitment and talent acquisition activities thereby reducing time and cost spent for character investigation. The application uses social media analytics to assign a social profile score. Unstructured text data are preprocessed to include only keywords which are relevant to the analysis. Word sense disambiguation is applied to determine the underlying meaning of the words. The bag-of-word is then checked for occurrence of associated words defined for each factor. Posts containing at least one occurrence of words associated with the factors are further tested for content polarity. Social character score is computed using proposed formula. The system recommends applicants based on skills and uses social character score for relevancy ranking of candidates relative to the job posts. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/245\"], \"author\": [\"Jovelyn Cuizon, Kent Ferolino\"], \"title\": [\"Social Media Character Assessment for Talent Selection using Natural Language Processing\"], \"date\": [\"2017-10-11 October 11 2017\"]}, {\"distance\": 0.22431766986846924, \"text\": [\"The study aimed to develop a software application to capture tourist activity information, extract movement patterns from the dataset through sequential pattern mining (SPM), and visualize spatiotemporal movement. Tourist activity information was captured through crowdsourced trajectory movements by scanning unique QR (Quick Response) codes for each visited tourist spots. The AprioriAll algorithm was used to find frequent trajectory patterns on tourist visits. The resulting maximal k-sequences and their subsequences represent the recommended trip itinerary. The spatial and temporal movements were visualized through a flow map and a heat map, respectively. The directed edges in the flow map show the recommended sequence of tourist sites to visit. The heat map shows the density of tourist visits in different areas at time intervals. The application was validated with selected tour planning experts to verify functional suitability, usability, and acceptability. Experimental results show positive indicators that the application met the users\\\\u00e2\\\\u20ac\\\\u2122 expectations. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/965\"], \"author\": [\"Maria Isabel R. Abucejo, Jovelyn C. Cuizon\"], \"title\": [\"Sequential Pattern Mining of Tourist Spatiotemporal Movement\"], \"date\": [\"2021-05-29 May 29 2021\"]}, {\"distance\": 0.38754430413246155, \"text\": [\"According to Clayton Christensen\\\\u00e2\\\\u20ac\\\\u2122s Disruptive Innovation Theory, upstarts eat up\\\\nmarket share often with innovative and more affordable products and soon become the new market leader. Christensen sees its relevance in many aspects of human endeavor including in education. Cheaper online education is said to be disruptive of colleges offering expensive classroom-based modes of instruction. This concern was highlighted in a forum held in Russia and attended by education leaders around the globe. Other scholars, however, dispute this. For them, it is the interruptions to the totalizing attempt of most scholarships and not the impact of technological innovations that is disrupting education. This paper supports this view, especially from the standpoint of developing countries long shackled by western-oriented paradigms. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/16\"], \"author\": [\"Jose D. Velez Jr.\"], \"title\": [\"Disrupting Education: Which between technological innovations and the \\\\u00e2\\\\u20ac\\\\u02dcincredulity towards metanarratives\\\\u00e2\\\\u20ac\\\\u2122 is disrupting education?\"], \"date\": [\"2015-06-30 June 30 2015\"]}, {\"distance\": 0.3896745443344116, \"text\": [\"The paper aimed to devise an alternative algorithm for solving system of linear\\\\ncongruences. This algorithm is an extension of the algebraic algorithm which is an alternative method for finding solutions in linear congruences. The basic idea of the technique is to convert the given linear congruences into linear equations and solve them algebraically. The advantage of this algorithm is the simplicity of its computations and its applicability to systems of linear congruences where the conditions of the Chinese Remainder Theorem that the moduli m1\\\\u00e2\\\\u20ac\\\\u00a6mn should be pairwise coprime is not satisfied. Some illustrative examples are given to show validity of this method for solving system of linear congruences. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/13\"], \"author\": [\"Polemer M. Cuarto\"], \"title\": [\"Algebraic Method for Solving System of Linear Congruences\"], \"date\": [\"2015-06-30 June 30 2015\"]}, {\"distance\": 0.41315609216690063, \"text\": [\"Atang (food offering) is an indigenous ritual for the dead in the Northern Philippines. The atang ritual is thought to be a part of the cultural and religious contexts of the Ilocano people. This research argued that the Ilocanos\\\\u00e2\\\\u20ac\\\\u2122 practice of atang ritual is compatible with the Catholic Doctrine on the Communion of Saints. This study utilized descriptive and contextual approaches in doing inculturation. It used the purposive sampling technique to Ilocano participants and discussed the development of doing a local theology of atang ritual in the faith of Ilocano Catholics. Results of the study revealed that the atang ritual has significant implications in the faith of the Ilocano Catholics in terms of the importance of remembering the dead as way to elaborate the doctrine of the communion of Saints. Thus, the concept of the communion of saints can be understood in the context of food offering for the dead. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/586\"], \"author\": [\"Jeff Clyde G. Corpuz\"], \"title\": [\"Death and Food Offering: The Ilocano \\\\u00e2\\\\u20ac\\\\u0153Atang\\\\u00e2\\\\u20ac\\\\u009d Ritual from a Contextual Theology\"], \"date\": [\"2020-06-30 June 30 2020\"]}, {\"distance\": 0.42345452308654785, \"text\": [\"This study determines why Gross Regional Domestic Product per Capita (GRDP) differs across regions in the Philippines vis-\\\\u00c3 -vis population rate, tertiary graduate rate, and crime rate as factors. Using Minitab, it examines the secondary data sets based on every group from various regions of the country. Findings show that educated human capital converge more in highly developed areas where risks matter less and safety measures exist. This results to a concentrated high GRDP in some. It further reveals that index and non-index crime rate is not a hindrance for a person to move in the area with many opportunities as long as life is not at stake. There were more educated individuals in regions with lower GRDP; however, they practiced and worked in highly developed areas and where law is enforced, eyeing a better condition of life. By implication, it results to concentration of educated labor force in areas with better opportunities. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/260\"], \"author\": [\"Joshua Edson G. Ordiz\"], \"title\": [\"Economic Decision across Regions of the Philippines\"], \"date\": [\"2017-12-31 December 31 2017\"]}, {\"distance\": 0.4244115352630615, \"text\": [\"This paper inquires into the problems concerning Filipino values and moral norms. Based on the interviews with the social science and philosophy scholars and the youth leaders in the Philippines, the study identifies the following problems: Filipino identity, distortion and dysfunctionalization, manifold ambivalence, dissonance, false justification and misuse, cynicism, and decline of moral courage. Analyzed based on Hans Kelsen\\\\u00e2\\\\u20ac\\\\u2122s concept of validity and efficacy, the problems prove to be radical given that the purported Filipino values system is actually a chaotic constellation of competing and conflicting pre-colonial, colonial, and postcolonial normative paradigms. Distorted, ambivalent, and dysfunctionalized, Filipino values and norms fail to provide effective normative guidelines. The proposed antidote of moral and values education is bound to be futile in the face of a severely mutilated social conscience. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/1211\"], \"author\": [\"Jiolito L. Benitez\"], \"title\": [\"An Inquiry into the Problems Concerning Filipino Values and Norms\"], \"date\": [\"2022-05-27 May 27 2022\"]}, {\"distance\": 0.42575603723526, \"text\": [\"The complexity of financial reporting highlights the need for professional skepticism among auditors. This study examined the relationship between auditors\\\\u2019 professional skepticism (PS) and thinking styles (TS) and explored what associations exist among PS, TS, and auditors\\\\u2019 sociodemographic attributes. Using snowball sampling, we surveyed 139 auditors in the Philippines using the Thinking Styles Inventory \\\\u2013 Revised II and the Professional Skepticism Scale. Confirmatory analysis using Kendall\\\\u2019s tau-B showed a strong correlation between PS and TS I (creativity-generating style), implying that it plays a prominent role in professional skepticism. Contrary to earlier studies, our study found only a moderate correlation between TS II (norm-favoring style) and PS. The k-Modes clustering algorithm revealed that auditors showing high PS were low-ranking, less-experienced licensed female CPAs in firms with global affiliations and manifesting creative-generating TS. These findings add to the accounting profession\\\\u2019s understanding of PS and may be of valuable help in cultivating PS among auditors. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/1320\"], \"author\": [\"Teovy Erdel Bongcales, Ariel Balunan, Loriemar Igot, Jurienel Mae Laude, JJ Jycka Mojados, Kristine June Uy\"], \"title\": [\"Auditors\\\\u2019 Professional Skepticism and Its Relationship with Their Thinking Styles\"], \"date\": [\"2022-12-31 December 31 2022\"]}, {\"distance\": 0.42833685874938965, \"text\": [\"Traffic flow mismanagement is a significant challenge in all countries especially in crowded cities. An alternative solution is to utilize smart technologies to predict traffic flow. In this study, frequency spectrum describing traffic sound characteristics is used as an indicator to predict the next five-minute vehicle density. Sound frequency and vehicle intensity are collected during a thirteen-hour data gathering. The collected sound intensity and frequency are then used to learn three machine-learning models - support vector machine, artificial neural network, and random forest and to predict vehicle intensity. It was found out that the performances of the three models based on root-mean-square-error values are 12.97, 16.01, and 10.67, respectively. These initial and satisfactory results pave a new way to predict traffic flow based on traffic sound characteristics which may serve as a better alternative to conventional features. link: https://rmrj.usjr.edu.ph/rmrj/index.php/RMRJ/article/view/982\"], \"author\": [\"Geoferleen Flores, Eduardo Jr. Piedad, Anzeneth Figueroa, Romari Tumamak, Nesrah Jane Marie Berdon\"], \"title\": [\"A Sound-based Machine Learning to Predict Traffic Vehicle Density\"], \"date\": [\"2021-05-29 May 29 2021\"]}]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Incorrect API key provided: sk-CNKfr***************************************CF3X. You can find your API key at https://platform.openai.com/account/api-keys.\n"
     ]
    }
   ],
   "source": [
    "question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7d616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc6ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285947f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75220d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
