{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b14c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56a933",
   "metadata": {},
   "source": [
    "Grammarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ca51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TOKENIZERS_PARALLELISM to true\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"grammarly/coedit-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"grammarly/coedit-xl\")\n",
    "\n",
    "def correct_grammar(text):\n",
    "# Encode the text input\n",
    "text = \"Fix grammatical errors in this sentence: \" + text\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "# Generate corrected text output\n",
    "outputs = model.generate(input_ids, max_length=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb284847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1bbb6689-12d9-4094-8fd2-5b8366c54c02)')' thrown while requesting HEAD https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327d2eca0ea24a43b7ddb2d9c1ef4557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feacd78e07604e99b34449dc491c8f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)l-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"mps\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check if MPS (Apple's Metal Performance Shaders) is available and set the device accordingly\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\").to(device)\n",
    "print(\"Tokenizer and model loaded successfully.\")\n",
    "\n",
    "def correct_grammar(text):\n",
    "    print(f\"Correcting grammar for text: {text}\")\n",
    "\n",
    "    # Format the prompt for grammar correction\n",
    "    prompt = f\"Correct the grammar: {text}\"\n",
    "    print(f\"Formatted prompt: {prompt}\")\n",
    "\n",
    "    # Encode the text input into model's input format\n",
    "    print(\"Encoding text...\")\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    print(\"Text encoded successfully.\")\n",
    "\n",
    "    # Generate corrected text output\n",
    "    print(\"Generating output...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "    print(\"Output generated.\")\n",
    "\n",
    "    # Decode the generated ids to text\n",
    "    print(\"Decoding output...\")\n",
    "    corrected_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f\"Decoded text: {corrected_text}\")\n",
    "\n",
    "    # Remove the initial prompt part from the output and return the corrected text\n",
    "    corrected_text = corrected_text.replace(prompt, \"\").strip()\n",
    "    print(f\"Final corrected text: {corrected_text}\")\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "# Example usage\n",
    "corrected_text = correct_grammar(\"I has a book.\")\n",
    "print(f\"Corrected Text: {corrected_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc09e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Decode and return the corrected text\n",
    "corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "return corrected_text\n",
    "spell = SpellChecker()\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "\n",
    "def assess_ocr_quality(text):\n",
    "# Number Heavy Check\n",
    "num_ratio = sum(c.isdigit() for c in text) / len(text) if text else 0\n",
    "if num_ratio > 0.5:\n",
    "    return 'number_heavy'\n",
    "\n",
    "# Basic Length Check\n",
    "if len(text) < 20:  # arbitrary minimum length\n",
    "    return 'unusable'\n",
    "\n",
    "# Spell Checking and Basic Grammar Analysis\n",
    "words = text.split()\n",
    "misspelled = spell.unknown(words)\n",
    "if len(misspelled) > 0.4 * len(words):  # if more than 40% of words are misspelled\n",
    "    return 'unusable'\n",
    "\n",
    "# Simple Coherence Check (can be improved with NLP techniques)\n",
    "# Example: Check for frequent occurrences of nonsensical character combinations\n",
    "if re.search(r'[^\\w\\s]', text):  # regex for non-word, non-space characters\n",
    "    return 'okay'\n",
    "\n",
    "# If none of the above, classify as good\n",
    "return 'good'\n",
    "\n",
    "source_dir = \"/Users/garfieldgreglim/Documents/JQ/Knowledgebase 2/images\"\n",
    "target_dirs = {\n",
    "'good': \"/Users/garfieldgreglim/Documents/JQ/Knowledgebase 2/image_ocrs/good_ocr\",\n",
    "'okay': \"/Users/garfieldgreglim/Documents/JQ/Knowledgebase 2/image_ocrs/okay_ocr\",\n",
    "'unusable': \"/Users/garfieldgreglim/Documents/JQ/Knowledgebase 2/image_ocrs/unusable_ocr\",\n",
    "'number_heavy': \"/Users/garfieldgreglim/Documents/JQ/Knowledgebase 2/image_ocrs/number_heavy_ocrs\"\n",
    "}\n",
    "if not os.path.exists(source_dir):\n",
    "print(\"Source directory does not exist.\")\n",
    "exit(1)\n",
    "\n",
    "for dir in target_dirs.values():\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(source_dir) if f.lower().endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "# Process each file with a progress bar\n",
    "for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "image_path = os.path.join(source_dir, filename)\n",
    "try:\n",
    "    text = pytesseract.image_to_string(Image.open(image_path))\n",
    "    text = correct_grammar(text)\n",
    "    quality = assess_ocr_quality(text)\n",
    "    with open(os.path.join(target_dirs[quality], filename + \".txt\"), \"w\") as text_file:\n",
    "        text_file.write(text)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b4a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
