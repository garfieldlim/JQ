{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1cf47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, DataType, CollectionSchema, FieldSchema, Collection, Partition, utility\n",
    "from pymilvus import Milvus, DataType, Collection, MilvusException\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from openai.embeddings_utils import get_embedding\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import fasttext\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7856f",
   "metadata": {},
   "source": [
    "Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32fdf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ca411",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53183d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_list = [\n",
    "    'text_collection',\n",
    "    'author_collection',\n",
    "    'title_collection',\n",
    "    'contact_collection',\n",
    "    'name_collection',\n",
    "    'position_collection',\n",
    "    'department_collection',\n",
    "    'date_collection',\n",
    "]\n",
    "fields_list = [\n",
    "    'text',\n",
    "    'author',\n",
    "    'title',\n",
    "    'contact',\n",
    "    'name',\n",
    "    'position',\n",
    "    'department',\n",
    "    'date',\n",
    "]\n",
    "collections_dict = {\n",
    "    \"text_collection\": [\"uuid\", \"text_id\", \"text\", \"embeds\", \"media\", \"link\", \"partition_name\"],\n",
    "    \"author_collection\": [\"uuid\", \"author\", \"embeds\", \"partition_name\"],\n",
    "    \"title_collection\": [\"uuid\", \"title\", \"embeds\", \"partition_name\"],\n",
    "    \"date_collection\": [\"uuid\", \"date\", \"embeds\", \"partition_name\"],\n",
    "    \"contact_collection\": [\"uuid\", \"contact\", \"embeds\", \"partition_name\"],\n",
    "    \"department_collection\": [\"uuid\", \"department\", \"embeds\", \"partition_name\"],\n",
    "    \"name_collection\": [\"uuid\", \"name\", \"embeds\", \"partition_name\"],\n",
    "    \"position_collection\": [\"uuid\", \"position\", \"embeds\", \"partition_name\"]\n",
    "}\n",
    "\n",
    "partitions = {\n",
    "    \"documents_partition\": [\"text_collection\", \"author_collection\", \"title_collection\", \"date_collection\"],\n",
    "    \"social_posts_partition\": [\"text_collection\", \"date_collection\"],\n",
    "    \"contacts_partition\": [\"name_collection\", \"text_collection\", \"contact_collection\", \"department_collection\"],\n",
    "    \"people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\",\"department_collection\"],\n",
    "    \"usjr_documents_partition\": [\"text_collection\", \"title_collection\"],\n",
    "    \"scs_documents_partition\" : [\"text_collection\"],\n",
    "    \"religious_admin_people_partition\": [\"text_collection\",\"name_collection\",\"position_collection\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63021bbb",
   "metadata": {},
   "source": [
    "Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e14b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the connection already exists\n",
    "if connections.has_connection('default'):\n",
    "    connections.remove_connection('default')  # Disconnect if it exists\n",
    "\n",
    "# Now, reconnect with your new configuration\n",
    "connections.connect(alias='default', host='localhost', port='19530')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa94f8",
   "metadata": {},
   "source": [
    "Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fefa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "    model = AutoModel.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "    # Prefix the text with 'query: '\n",
    "    text = 'query: ' + text\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Average pool the last hidden states and apply the attention mask\n",
    "    embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # Convert tensor to list\n",
    "    embeddings_list = embeddings.tolist()\n",
    "\n",
    "    return embeddings_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde8963",
   "metadata": {},
   "source": [
    "Symbol remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b83715d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbcf0d",
   "metadata": {},
   "source": [
    "Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff960efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_query(query):\n",
    "    return get_embedding(query.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a584312",
   "metadata": {},
   "source": [
    "Search collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f4fa838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_collections(vectors, partition_names):\n",
    "    results_dict = {}\n",
    "    search_params = {\n",
    "        \"metric_type\": \"L2\",  # Distance metric, can be L2, IP (Inner Product), etc.\n",
    "        \"offset\": 0,\n",
    "    }\n",
    "    for name in fields_list:\n",
    "        try:\n",
    "            collection = Collection(f\"{name}_collection\")\n",
    "            collection.load()\n",
    "            results = collection.search([vectors], \"embeds\", param=search_params, limit=10, partition_names=partition_names)\n",
    "            results_dict[name] = results\n",
    "        except MilvusException as e:\n",
    "            if 'partition name' in str(e) and 'not found' in str(e):\n",
    "                print(f\"Partition '{partition_names}' not found in collection '{name}', skipping...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise e  # if it's a different kind of MilvusException, we still want to raise it\n",
    "    for hit in results[0]:  # Changed 'result' to 'results'\n",
    "        print(f\"ID: {hit.id}, Distance: {hit.distance}\")\n",
    "\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c6bfe",
   "metadata": {},
   "source": [
    "Check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9f830cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collection_dimension(collection):\n",
    "    collection_params = collection.schema\n",
    "    vector_field = [field for field in collection_params.fields if field.dtype == DataType.FLOAT_VECTOR][0]\n",
    "    print(f\"Dimension of vectors in collection '{collection.name}': {vector_field.params['dim']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d53de",
   "metadata": {},
   "source": [
    "Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc2d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results_dict):\n",
    "    json_results = {}\n",
    "    for collection_name, result in results_dict.items():\n",
    "        for query_hits in result:\n",
    "            for hit in query_hits:\n",
    "                print(hit.entity.get(entity_id))\n",
    "    \n",
    "    for collection_name, result in results_dict.items():\n",
    "        for query_hits in result:\n",
    "            for hit in query_hits:\n",
    "                if collection_name == 'text':\n",
    "                    id_field = 'entity_id'\n",
    "                    id_value = hit.entity.get('text_id')\n",
    "                else:\n",
    "                    id_field = 'uuid'\n",
    "                    id_value = hit.id\n",
    "                \n",
    "                # Create the result dictionary\n",
    "                result_dict = {\n",
    "                    id_field: id_value,\n",
    "                    \"distance\": hit.distance,\n",
    "                    \"collection\": collection_name\n",
    "                }\n",
    "\n",
    "                # If the id_value is already in the results and the new distance is greater, skip\n",
    "                if id_value in json_results and json_results[id_value][\"distance\"] < hit.distance:\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, update/insert the result\n",
    "                json_results[id_value] = result_dict\n",
    "                \n",
    "            json_results_list = list(json_results.values())\n",
    "            json_results_sorted = sorted(json_results_list, key=lambda x: x['distance'])\n",
    "    \n",
    "    return json_results_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02567d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_results(json_results_sorted, partition_names):\n",
    "    # Load all collections beforehand\n",
    "    collections = {name: Collection(f\"{name}_collection\") for name in fields_list}\n",
    "\n",
    "    # Create a list of entity IDs for the query\n",
    "    entity_ids = [result[\"entity_id\"] for result in json_results_sorted]\n",
    "    print(\"ENTITY: \", entity_ids)\n",
    "    # Preparing an empty dictionary for each field in the results\n",
    "    for result in json_results_sorted:\n",
    "        for name in fields_list:\n",
    "            result[name] = []\n",
    "\n",
    "    # Query for all relevant records at once\n",
    "    for name, collection in collections.items():\n",
    "        try:\n",
    "            # Prepare the query\n",
    "            output_fields = []\n",
    "            if name == 'text':\n",
    "                query_field = \"text_id\"\n",
    "                output_fields = [name, 'text_id']\n",
    "            else:\n",
    "                query_field = \"uuid\"\n",
    "                output_fields = [name]\n",
    "\n",
    "            query = f\"{query_field} in {entity_ids}\"\n",
    "\n",
    "            query_results = collection.query(\n",
    "                expr=query, \n",
    "                offset=0, \n",
    "                limit=10, \n",
    "                partition_names=[partition_names], \n",
    "                output_fields=output_fields, \n",
    "                consistency_level=\"Strong\"\n",
    "            )\n",
    "\n",
    "            # Append the results to the relevant fields in the results dictionary\n",
    "            for query_result in query_results:\n",
    "                for result in json_results_sorted:\n",
    "                    if (name == 'text' and result[\"entity_id\"] == query_result[\"text_id\"]) or (name != 'text' and result[\"entity_id\"] == query_result[\"uuid\"]):\n",
    "                        result[name].append(query_result[name])\n",
    "            final_results = []\n",
    "            for result in json_results_sorted:\n",
    "                obj = {}\n",
    "                for item in result:\n",
    "                    # If item is not 'entity_id' or 'distance' and the item's value is not empty\n",
    "                    if item not in ['entity_id', 'collection'] and result[item]:\n",
    "                        obj[item] = result[item]\n",
    "                final_results.append(obj)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with collection {name}: {str(e)}\")\n",
    "    return final_results[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e4f87e96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, string_json):\n",
    "    # Format the input as per the desired conversation format\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': \"\"\"You are Josenian Quiri. University of San Jose- Recoletos' general knowledge base assistant. Refer to yourself as JQ. If there are links, give the link as well.\"\"\"},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "        {'role': 'system', 'content': f'Here is the database JSON from your knowledge base (note: select only the correct answer): \\n{string_json[:4500]}'},\n",
    "        {'role': 'user', 'content': ''}\n",
    "    ]\n",
    "    \n",
    "    # Convert the conversation to a string\n",
    "    conversation_str = ''.join([f'{item[\"role\"]}: {item[\"content\"]}\\n' for item in conversation])\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-4\",\n",
    "      messages=conversation,\n",
    "      temperature=1,\n",
    "      max_tokens=1000,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    # Extract the generated response from the API's response\n",
    "    generated_text = response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "    # Return the response\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f84d78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attribute = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/clf_attribute.pkl')\n",
    "clf_partition = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/clf_partition.pkl')\n",
    "\n",
    "# load encoders\n",
    "le_attribute = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/le_attribute.pkl')\n",
    "le_partition = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/le_partition.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "30a35b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attribute = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/clf_attribute.pkl')\n",
    "clf_partition = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/clf_partition.pkl')\n",
    "\n",
    "# load encoders\n",
    "le_attribute = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/le_attribute.pkl')\n",
    "le_partition = joblib.load('/Users/garfieldgreglim/Documents/JQ/Milvus Database/classifier_model/le_partition.pkl')\n",
    "\n",
    "def predict_attribute(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_attribute.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_attribute.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_attribute.inverse_transform(ranked_classes)\n",
    "\n",
    "def predict_partition(embeds):\n",
    "    # transform input to the right format\n",
    "    X = np.stack([embeds])\n",
    "\n",
    "    # predict probabilities across all possible labels\n",
    "    probas = clf_partition.predict_proba(X)[0]\n",
    "\n",
    "    # get class labels in descending order of probability\n",
    "    classes = clf_partition.classes_\n",
    "    ranked_classes = [x for _, x in sorted(zip(probas, classes), reverse=True)]\n",
    "\n",
    "    # return the names instead of the encoded labels\n",
    "    return le_partition.inverse_transform(ranked_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f6f2635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_partitions(vectors):\n",
    "    return ['documents_partition','social_posts_partition', 'people_partition', \"contacts_partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7dd95a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer():\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"You: \")\n",
    "            if not prompt:\n",
    "                print(\"No input provided. Try again.\")\n",
    "                continue\n",
    "            vectors = vectorize_query(prompt)\n",
    "            if vectors is None:\n",
    "                print(\"No vectors returned. Check your vectorize_query function.\")\n",
    "                continue\n",
    "            ranked_partitions = ranking_partitions(vectors)\n",
    "            if ranked_partitions is None:\n",
    "                print(\"No ranked_partitions returned. Check your ranking_partitions function.\")\n",
    "                continue\n",
    "            partition = 0\n",
    "            correct = 0\n",
    "            display(ranked_partitions)\n",
    "            display(ranked_partitions[partition])\n",
    "            while correct != 1:\n",
    "                results_dict = search_collections(vectors, [ranked_partitions[partition]])\n",
    "                print(results_dict)\n",
    "                if results_dict is None:\n",
    "                    print(\"No results returned. Check your search_collections function.\")\n",
    "                    break\n",
    "                json_results_sorted = process_results(results_dict)\n",
    "                \n",
    "                if json_results_sorted is None:\n",
    "                    print(\"No sorted results returned. Check your process_results function.\")\n",
    "                    break\n",
    "                final_results = populate_results(json_results_sorted, ranked_partitions[partition])\n",
    "                if final_results is None:\n",
    "                    print(\"No final results returned. Check your populate_results function.\")\n",
    "                    break\n",
    "                print(\"FINAL RESULTS: \", final_results)\n",
    "                string_json = json.dumps(final_results)\n",
    "                display(string_json)\n",
    "                generated_text = generate_response(prompt, string_json)\n",
    "                if generated_text is None:\n",
    "                    print(\"No response generated. Check your generate_response function.\")\n",
    "                    break\n",
    "                print(f\"JQ: {generated_text}\")\n",
    "                correct = input(\"Is the answer correct? 1-Y, 0-N: \")\n",
    "                if correct not in ['0', '1']:\n",
    "                    print(\"Invalid input. Try again.\")\n",
    "                elif partition <= 3 :\n",
    "                    partition = partition + 1\n",
    "                else:\n",
    "                    partition = 0\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "342f9f0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00558ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: emiliano\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['documents_partition',\n",
       " 'social_posts_partition',\n",
       " 'people_partition',\n",
       " 'contacts_partition']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'documents_partition'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:19:47.325503', 'RPC error': '2023-07-31 00:19:47.328018'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:19:47.342702', 'RPC error': '2023-07-31 00:19:47.346187'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:19:47.363538', 'RPC error': '2023-07-31 00:19:47.366523'}>\n",
      "RPC error: [search], <MilvusException: (code=1, message=partition name documents_partition not found)>, <Time:{'RPC start': '2023-07-31 00:19:47.383861', 'RPC error': '2023-07-31 00:19:47.387152'}>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition '['documents_partition']' not found in collection 'contact', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'name', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'position', skipping...\n",
      "Partition '['documents_partition']' not found in collection 'department', skipping...\n",
      "ID: 8f4c5c04-f0ce-44c7-ba08-510697fb62b7, Distance: 0.4838462471961975\n",
      "ID: e33cd284-4dd7-4997-a52f-3c18a9503725, Distance: 0.4880058169364929\n",
      "ID: ba43f93b-dc34-4b4b-b96d-7c99ed6e0fb4, Distance: 0.4880058169364929\n",
      "ID: 8e1d49d2-06e5-4373-a83a-a5e628574a44, Distance: 0.4880058169364929\n",
      "ID: 7efc82bb-ac5b-4596-8724-d370980ba2df, Distance: 0.4880058169364929\n",
      "ID: 278455c0-0e77-4725-b072-4b61496408a6, Distance: 0.4880058169364929\n",
      "ID: 96b40848-dead-4a06-bb67-befb5071711f, Distance: 0.48856762051582336\n",
      "ID: 189348b6-2ee5-4487-99a0-b62a80538d5a, Distance: 0.49010908603668213\n",
      "ID: a9e24e6e-cfd0-4bc5-a30a-8786816b040e, Distance: 0.4901444613933563\n",
      "ID: e47c4ba0-6b5a-497f-ab54-4ed4a86cb850, Distance: 0.49074333906173706\n",
      "{'text': <pymilvus.orm.search.SearchResult object at 0x2b45a4450>, 'author': <pymilvus.orm.search.SearchResult object at 0x2d863fc10>, 'title': <pymilvus.orm.search.SearchResult object at 0x33eb63ed0>, 'date': <pymilvus.orm.search.SearchResult object at 0x2dc99dcd0>}\n",
      "An error occurred: name 'entity_id' is not defined\n"
     ]
    }
   ],
   "source": [
    "question_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7d616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc6ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285947f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75220d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb2119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ece48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a2efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043421b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
